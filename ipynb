import pandas as pd
import xgboost as xgb
from sklearn.cluster import KMeans, DBSCAN
from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.utils import resample
import numpy as np
import matplotlib.pyplot as plt

# Load dataset
data = pd.read_excel("/content/Cleaned_data_after_removing_outliers.xlsx")

# Step 1: Remove rows with missing Lat, Long_, or CFR
data_cleaned = data.dropna(subset=['Lat', 'Long', 'CFR']).reset_index(drop=True)

# Step 2: Apply KMeans clustering
kmeans = KMeans(n_clusters=min(5, len(data_cleaned)), init='k-means++', random_state=42, n_init=10)
data_cleaned['cluster'] = kmeans.fit_predict(data_cleaned[['Lat', 'Long']])

# Step 3: Remove outliers within each cluster using DBSCAN
cleaned_data = pd.DataFrame()
for cluster in data_cleaned['cluster'].unique():
    cluster_data = data_cleaned[data_cleaned['cluster'] == cluster]
    dbscan = DBSCAN(eps=10.0, min_samples=10, n_jobs=-1)
    cluster_data.loc[:, 'dbscan_label'] = dbscan.fit_predict(cluster_data[['Lat', 'Long']])
    cluster_data = cluster_data[cluster_data['dbscan_label'] != -1]
    cleaned_data = pd.concat([cleaned_data, cluster_data])

# Step 4: Feature Engineering
cleaned_data['lat_long_interaction'] = cleaned_data['Lat'] * cleaned_data['Long']
cleaned_data['lat_CFR_interaction'] = cleaned_data['Lat'] * cleaned_data['CFR']
cleaned_data['long_CFR_interaction'] = cleaned_data['Long'] * cleaned_data['CFR']
cleaned_data['health_CFR_interaction'] = cleaned_data['Healthcare_Workforce_per_1000'] * cleaned_data['CFR']
cleaned_data['pop_CFR_interaction'] = cleaned_data['Population_Index'] * cleaned_data['CFR']
cleaned_data['neh_CFR_interaction'] = cleaned_data['nearest_hotspot'] * cleaned_data['CFR']

# One-hot encode the "Cluster" column
cleaned_data_encoded = pd.get_dummies(cleaned_data, columns=['Cluster'], prefix='cluster', dtype=int)
cluster_feature_columns = [col for col in cleaned_data_encoded.columns if col.startswith('cluster_')]

# Step 5: Prepare Features and Target
X = cleaned_data_encoded[['Lat', 'Long', 'CFR', 'Population_Index', 'Healthcare_Workforce_per_1000', 'nearest_hotspot',
                          'lat_long_interaction', 'lat_CFR_interaction', 'long_CFR_interaction',
                          'health_CFR_interaction', 'pop_CFR_interaction', 'neh_CFR_interaction'] + cluster_feature_columns]

# Handle missing values
train_data = cleaned_data_encoded[cleaned_data_encoded['Deaths_x'].notnull()]
X_train_full = train_data[X.columns]
y_train_full = np.log1p(train_data['Deaths_x'])  # Apply log transformation

# Log transform selected features
for col in ['Population_Index', 'Healthcare_Workforce_per_1000', 'nearest_hotspot']:
    X_train_full[col] = np.log1p(X_train_full[col])

# Step 6: Handle Imbalance
X_resampled, y_resampled = resample(X_train_full, y_train_full, replace=True, n_samples=len(X_train_full), random_state=42)

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.1, random_state=42)

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Step 7: Hyperparameter Tuning using RandomizedSearchCV
param_grid = {
    'n_estimators': [100, 300, 500],
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [4, 6, 8],
    'min_child_weight': [1, 3, 5],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'reg_alpha': [0, 1.0, 10.0],
    'reg_lambda': [0, 1.0, 10.0]
}

random_search = RandomizedSearchCV(xgb.XGBRegressor(objective='reg:squarederror', random_state=42),
                                   param_distributions=param_grid, cv=2, scoring='r2', n_jobs=-1, verbose=1, n_iter=10, random_state=42)
random_search.fit(X_train_scaled, y_train)

best_params = random_search.best_params_

# Train XGBoost Model with Optimized Hyperparameters
xgboost_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42, **best_params)
xgboost_model.fit(X_train_scaled, y_train)

# Evaluate the model
y_pred_test_log = xgboost_model.predict(X_test_scaled)
y_pred_test = np.expm1(y_pred_test_log)  # Invert log transformation
y_test_actual = np.expm1(y_test)

mae = mean_absolute_error(y_test_actual, y_pred_test)
mse = mean_squared_error(y_test_actual, y_pred_test)
rmse = np.sqrt(mse)
r2 = r2_score(y_test_actual, y_pred_test)

# Print metrics
print(f"MAE (Mean Absolute Error): {mae}")
print(f"MSE (Mean Squared Error): {mse}")
print(f"RMSE (Root Mean Squared Error): {rmse}")
print(f"R² (R-Squared): {r2}")

# Cross-validation
cv_scores = cross_val_score(xgboost_model, X_train_scaled, y_train, cv=10, scoring='r2')
print("Cross-Validation R² Scores:", cv_scores)
print("Mean R²:", np.mean(cv_scores))

# Feature importance
xgb.plot_importance(xgboost_model, importance_type='weight', max_num_features=10, title='Top 10 Feature Importances')
plt.show()

# Retrieve feature importance as a DataFrame
feature_importance = xgboost_model.get_booster().get_score(importance_type='weight')
feature_importance_df = pd.DataFrame({
    'Feature': feature_importance.keys(),
    'Importance': feature_importance.values()
}).sort_values(by='Importance', ascending=False)

print("Feature Importance:")
print(feature_importance_df)

actual_vs_predicted = pd.DataFrame({'Actual': y_test_actual, 'Predicted': y_pred_test})
actual_vs_predicted.to_csv("actual_vs_predicted.csv", index=False)
print("Actual vs. Predicted values saved as 'actual_vs_predicted.csv'.")
# Add these lines after the existing code to create the plot
plt.figure(figsize=(10, 6))
plt.scatter(y_test_actual, y_pred_test, alpha=0.5)
plt.plot([y_test_actual.min(), y_test_actual.max()], [y_test_actual.min(), y_test_actual.max()], 'r--', lw=2)
plt.xlabel('Actual Deaths')
plt.ylabel('Predicted Deaths')
plt.title('Actual vs Predicted Deaths')
plt.tight_layout()
plt.savefig('actual_vs_predicted.png')
plt.show()

# Additional diagnostic plot
plt.figure(figsize=(10, 6))
plt.scatter(y_test_actual, y_pred_test - y_test_actual, alpha=0.5)
plt.xlabel('Actual Deaths')
plt.ylabel('Residuals (Predicted - Actual)')
plt.title('Residual Plot')
plt.axhline(y=0, color='r', linestyle='--')
plt.tight_layout()
plt.savefig('residual_plot.png')
plt.show()

# Print correlation coefficient
from scipy.stats import pearsonr
correlation, _ = pearsonr(y_test_actual, y_pred_test)
print(f"Correlation between Actual and Predicted: {correlation:.4f}")
